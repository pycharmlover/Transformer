#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Translator: ä½¿ç”¨è®­ç»ƒå¥½çš„Transformeræ¨¡å‹è¿›è¡Œç¿»è¯‘
"""

import torch
import argparse
from transformers import AutoTokenizer
from src.models.transformer import TransformerSeq2Seq
import os


class Translator:
    """å¤šè¯­è¨€ç¿»è¯‘å™¨"""
    
    # æ”¯æŒçš„è¯­è¨€
    SUPPORTED_LANGUAGES = {
        'en': 'English',
        'de': 'German'
    }
    
    def __init__(self, model_path, device='cuda' if torch.cuda.is_available() else 'cpu'):
        """
        åˆå§‹åŒ–ç¿»è¯‘å™¨
        
        Args:
            model_path: æ¨¡å‹æƒé‡æ–‡ä»¶è·¯å¾„
            device: è¿è¡Œè®¾å¤‡ ('cuda', 'cpu', æˆ– 'auto')
        """
        # å¤„ç† 'auto' è®¾å¤‡é€‰æ‹©
        if device == 'auto':
            device = 'cuda' if torch.cuda.is_available() else 'cpu'
        
        self.device = device
        print(f"ğŸ”§ åŠ è½½æ¨¡å‹ä¸­...")
        print(f"   è®¾å¤‡: {device}")
        print(f"   æ¨¡å‹è·¯å¾„: {model_path}")
        
        # åŠ è½½æ¨¡å‹checkpoint
        checkpoint = torch.load(model_path, map_location=device)
        
        # è·å–æ¨¡å‹å‚æ•°
        self.args = argparse.Namespace(**checkpoint['args'])
        
        # åŠ è½½tokenizerï¼ˆå°è¯•åŠ è½½è®­ç»ƒæ—¶ä¿å­˜çš„åŒ…å«è¯­è¨€æ ‡è®°çš„ç‰ˆæœ¬ï¼‰
        tokenizer_with_tags_path = "data/tokenizer_with_tags"
        if os.path.exists(tokenizer_with_tags_path):
            print(f"   âœ… ä½¿ç”¨è®­ç»ƒæ—¶ä¿å­˜çš„tokenizerï¼ˆå«è¯­è¨€æ ‡è®°ï¼‰")
            print(f"      è·¯å¾„: {tokenizer_with_tags_path}")
            self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_with_tags_path, local_files_only=True)
        else:
            print(f"   âš ï¸  æœªæ‰¾åˆ° {tokenizer_with_tags_path}")
            print(f"   ä½¿ç”¨åŸå§‹tokenizerå¹¶æ‰‹åŠ¨æ·»åŠ è¯­è¨€æ ‡è®°: {self.args.tokenizer_path}")
            self.tokenizer = AutoTokenizer.from_pretrained(self.args.tokenizer_path)
            # æ·»åŠ è¯­è¨€æ ‡è®°ï¼ˆå¿…é¡»ä¸è®­ç»ƒæ—¶ä¸€è‡´ï¼‰
            special_tokens = ['<2en>', '<2de>']  # è‹±è¯­å’Œå¾·è¯­
            num_added = self.tokenizer.add_special_tokens({'additional_special_tokens': special_tokens})
            if num_added > 0:
                print(f"   Added {num_added} language tags: {special_tokens}")
        
        # éªŒè¯ç‰¹æ®Štokenæ˜¯å¦æ­£ç¡®
        print("\n   ğŸ” éªŒè¯è¯­è¨€æ ‡è®°:")
        for tag in ['<2en>', '<2de>']:  # è‹±è¯­å’Œå¾·è¯­
            token_id = self.tokenizer.convert_tokens_to_ids(tag)
            decoded = self.tokenizer.decode([token_id])
            print(f"      {tag} â†’ ID={token_id} â†’ decode='{decoded}'")
        
        # è·å–æ­£ç¡®çš„vocab_sizeï¼ˆä½¿ç”¨len()è€Œä¸æ˜¯.vocab_sizeå±æ€§ï¼‰
        # tokenizer.vocab_sizeå¯èƒ½è¿”å›åŸå§‹å¤§å°ï¼Œlen(tokenizer)è¿”å›å®é™…å¤§å°
        actual_vocab_size = len(self.tokenizer)
        print(f"\n   ğŸ“Š Tokenizerä¿¡æ¯:")
        print(f"      tokenizer.vocab_size = {self.tokenizer.vocab_size}")
        print(f"      len(tokenizer) = {actual_vocab_size}")
        print(f"      ä½¿ç”¨vocab_size = {actual_vocab_size}")
        
        # åˆå§‹åŒ–æ¨¡å‹ï¼ˆä½¿ç”¨len(tokenizer)è€Œä¸æ˜¯tokenizer.vocab_sizeï¼‰
        self.model = TransformerSeq2Seq(
            vocab_size=actual_vocab_size,
            d_model=self.args.d_model,
            num_layers=self.args.num_layers,
            num_heads=self.args.num_heads,
            d_ff=self.args.d_ff,
            max_len=max(self.args.max_src_len, self.args.max_tgt_len),
            dropout=self.args.dropout,
            share_embeddings=False,
            attention_type=self.args.attention_type
        ).to(device)
        
        # åŠ è½½æ¨¡å‹æƒé‡
        self.model.load_state_dict(checkpoint['model_state'])
        self.model.eval()
        
        print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼")
        print(f"   å‚æ•°é‡: {sum(p.numel() for p in self.model.parameters()) / 1e6:.1f}M")
        print(f"   æ³¨æ„åŠ›æœºåˆ¶: {self.args.attention_type}")
        print(f"   æ”¯æŒçš„è¯­è¨€: {', '.join([f'{k} ({v})' for k, v in self.SUPPORTED_LANGUAGES.items()])}")
        print()
    
    def translate(self, text, src_lang='en', tgt_lang='zh', beam_size=5, max_length=128):
        """
        ç¿»è¯‘æ–‡æœ¬
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            src_lang: æºè¯­è¨€ä»£ç  ('en', 'zh', 'ja')
            tgt_lang: ç›®æ ‡è¯­è¨€ä»£ç  ('en', 'zh', 'ja')
            beam_size: Beam searchå®½åº¦
            max_length: æœ€å¤§ç”Ÿæˆé•¿åº¦
            
        Returns:
            dict: åŒ…å«ä»¥ä¸‹å­—æ®µçš„å­—å…¸
                - 'input': è¾“å…¥æ–‡æœ¬
                - 'translation': ç¿»è¯‘åçš„æ–‡æœ¬
                - 'src_lang': æºè¯­è¨€
                - 'tgt_lang': ç›®æ ‡è¯­è¨€
        """
        if src_lang not in self.SUPPORTED_LANGUAGES:
            raise ValueError(f"ä¸æ”¯æŒçš„æºè¯­è¨€: {src_lang}. æ”¯æŒçš„è¯­è¨€: {list(self.SUPPORTED_LANGUAGES.keys())}")
        if tgt_lang not in self.SUPPORTED_LANGUAGES:
            raise ValueError(f"ä¸æ”¯æŒçš„ç›®æ ‡è¯­è¨€: {tgt_lang}. æ”¯æŒçš„è¯­è¨€: {list(self.SUPPORTED_LANGUAGES.keys())}")
        if src_lang == tgt_lang:
            return {
                'input': text,
                'translation': text,
                'src_lang': src_lang,
                'tgt_lang': tgt_lang
            }
        
        print(f"ğŸŒ ç¿»è¯‘: {self.SUPPORTED_LANGUAGES[src_lang]} â†’ {self.SUPPORTED_LANGUAGES[tgt_lang]}")
        print(f"ğŸ“ è¾“å…¥: {text}")
        
        # åœ¨è¾“å…¥å‰æ·»åŠ ç›®æ ‡è¯­è¨€æ ‡è®°
        text_with_tag = f"<2{tgt_lang}> {text}"
        print(f"   (æ·»åŠ è¯­è¨€æ ‡è®°: <2{tgt_lang}>)")
        
        # Tokenizeè¾“å…¥
        src_tokens = self.tokenizer(
            text_with_tag,
            max_length=self.args.max_src_len,
            truncation=True,
            padding='max_length',
            return_tensors='pt'
        )
        
        src_input_ids = src_tokens['input_ids'].to(self.device)
        src_attention_mask = src_tokens['attention_mask'].to(self.device)
        
        # ä½¿ç”¨Beam Searchç”Ÿæˆç¿»è¯‘
        with torch.no_grad():
            if beam_size > 1:
                translation_ids = self._beam_search(
                    src_input_ids, 
                    src_attention_mask, 
                    beam_size=beam_size, 
                    max_length=max_length
                )
            else:
                translation_ids = self._greedy_decode(
                    src_input_ids, 
                    src_attention_mask, 
                    max_length=max_length
                )
        
        # è§£ç è¾“å‡º
        translation = self.tokenizer.decode(
            translation_ids[0], 
            skip_special_tokens=True, 
            clean_up_tokenization_spaces=True
        )
        
        print(f"âœ… è¾“å‡º: {translation}")
        print()
        
        # è¿”å›å­—å…¸æ ¼å¼ï¼ŒåŒ…å«è¾“å…¥å’Œè¾“å‡º
        return {
            'input': text,
            'translation': translation,
            'src_lang': src_lang,
            'tgt_lang': tgt_lang
        }
    
    def _greedy_decode(self, src_input_ids, src_attention_mask, max_length=128):
        """
        Greedyè§£ç ï¼ˆé€tokenç”Ÿæˆï¼Œé€‰æ‹©æ¦‚ç‡æœ€é«˜çš„ï¼‰
        
        Args:
            src_input_ids: æºè¯­è¨€è¾“å…¥ [batch_size, src_len]
            src_attention_mask: æºè¯­è¨€attention mask
            max_length: æœ€å¤§ç”Ÿæˆé•¿åº¦
            
        Returns:
            ç”Ÿæˆçš„token ids [batch_size, tgt_len]
        """
        batch_size = src_input_ids.size(0)
        
        # åˆå§‹åŒ–ï¼šåªæœ‰[CLS]
        tgt_input_ids = torch.full(
            (batch_size, 1), 
            self.tokenizer.cls_token_id, 
            dtype=torch.long, 
            device=self.device
        )
        
        for _ in range(max_length - 1):
            # å‰å‘ä¼ æ’­
            outputs = self.model(
                src_input_ids=src_input_ids,
                tgt_input_ids=tgt_input_ids,
                src_attention_mask=src_attention_mask,
                src_pad_id=self.tokenizer.pad_token_id,
                tgt_pad_id=self.tokenizer.pad_token_id
            )
            
            # è·å–æœ€åä¸€ä¸ªä½ç½®çš„logits
            next_token_logits = outputs.logits[:, -1, :]  # [batch_size, vocab_size]
            
            # é€‰æ‹©æ¦‚ç‡æœ€é«˜çš„token
            next_token = torch.argmax(next_token_logits, dim=-1, keepdim=True)  # [batch_size, 1]
            
            # æ‹¼æ¥åˆ°å·²ç”Ÿæˆçš„åºåˆ—
            tgt_input_ids = torch.cat([tgt_input_ids, next_token], dim=1)
            
            # å¦‚æœç”Ÿæˆäº†[SEP]ï¼Œåœæ­¢
            if (next_token == self.tokenizer.sep_token_id).all():
                break
        
        return tgt_input_ids
    
    def _beam_search(self, src_input_ids, src_attention_mask, beam_size=5, max_length=128):
        """
        Beam Searchè§£ç 
        
        Args:
            src_input_ids: æºè¯­è¨€è¾“å…¥ [1, src_len]
            src_attention_mask: æºè¯­è¨€attention mask
            beam_size: beamå®½åº¦
            max_length: æœ€å¤§ç”Ÿæˆé•¿åº¦
            
        Returns:
            æœ€ä½³ç”Ÿæˆåºåˆ— [1, tgt_len]
        """
        batch_size = src_input_ids.size(0)
        assert batch_size == 1, "Beam searchåªæ”¯æŒbatch_size=1"
        
        # åˆå§‹åŒ–beam
        # æ¯ä¸ªbeam: (sequence, score)
        beams = [(torch.full((1, 1), self.tokenizer.cls_token_id, dtype=torch.long, device=self.device), 0.0)]
        
        for step in range(max_length - 1):
            all_candidates = []
            
            for seq, score in beams:
                # å¦‚æœå·²ç»ç”Ÿæˆäº†[SEP]ï¼Œä¸å†æ‰©å±•
                if seq[0, -1].item() == self.tokenizer.sep_token_id:
                    all_candidates.append((seq, score))
                    continue
                
                # å‰å‘ä¼ æ’­
                outputs = self.model(
                    src_input_ids=src_input_ids,
                    tgt_input_ids=seq,
                    src_attention_mask=src_attention_mask,
                    src_pad_id=self.tokenizer.pad_token_id,
                    tgt_pad_id=self.tokenizer.pad_token_id
                )
                
                # è·å–ä¸‹ä¸€ä¸ªtokençš„logæ¦‚ç‡
                next_token_logits = outputs.logits[0, -1, :]  # [vocab_size]
                log_probs = torch.log_softmax(next_token_logits, dim=-1)
                
                # è·å–top-kä¸ªå€™é€‰
                topk_log_probs, topk_indices = torch.topk(log_probs, beam_size)
                
                # æ‰©å±•beam
                for log_prob, token_id in zip(topk_log_probs, topk_indices):
                    new_seq = torch.cat([seq, token_id.unsqueeze(0).unsqueeze(0)], dim=1)
                    new_score = score + log_prob.item()
                    all_candidates.append((new_seq, new_score))
            
            # é€‰æ‹©scoreæœ€é«˜çš„beam_sizeä¸ªå€™é€‰
            beams = sorted(all_candidates, key=lambda x: x[1], reverse=True)[:beam_size]
            
            # å¦‚æœæ‰€æœ‰beaméƒ½ç»“æŸäº†ï¼Œåœæ­¢
            if all(seq[0, -1].item() == self.tokenizer.sep_token_id for seq, _ in beams):
                break
        
        # è¿”å›å¾—åˆ†æœ€é«˜çš„åºåˆ—
        best_seq, _ = beams[0]
        return best_seq
    
    def translate_batch(self, texts, src_lang='en', tgt_lang='zh', beam_size=5, max_length=128):
        """
        æ‰¹é‡ç¿»è¯‘
        
        Args:
            texts: æ–‡æœ¬åˆ—è¡¨
            src_lang: æºè¯­è¨€ä»£ç 
            tgt_lang: ç›®æ ‡è¯­è¨€ä»£ç 
            beam_size: Beam searchå®½åº¦ï¼ˆè®¾ä¸º1ä½¿ç”¨greedyï¼‰
            max_length: æœ€å¤§ç”Ÿæˆé•¿åº¦
            
        Returns:
            ç¿»è¯‘ç»“æœåˆ—è¡¨
        """
        translations = []
        for text in texts:
            translation = self.translate(text, src_lang, tgt_lang, beam_size, max_length)
            translations.append(translation)
        return translations


def main():
    """å‘½ä»¤è¡Œæ¥å£"""
    parser = argparse.ArgumentParser(description='Transformerå¤šè¯­è¨€ç¿»è¯‘å™¨')
    parser.add_argument('--model_path', type=str, default='results/best_model.pt',
                       help='æ¨¡å‹æƒé‡æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--text', type=str, required=True,
                       help='è¦ç¿»è¯‘çš„æ–‡æœ¬')
    parser.add_argument('--src_lang', type=str, default='en',
                       choices=['en', 'zh', 'ja'],
                       help='æºè¯­è¨€ (en=è‹±è¯­, zh=ä¸­æ–‡, ja=æ—¥è¯­)')
    parser.add_argument('--tgt_lang', type=str, default='zh',
                       choices=['en', 'zh', 'ja'],
                       help='ç›®æ ‡è¯­è¨€ (en=è‹±è¯­, zh=ä¸­æ–‡, ja=æ—¥è¯­)')
    parser.add_argument('--beam_size', type=int, default=5,
                       help='Beam searchå®½åº¦ (1=greedy decoding)')
    parser.add_argument('--max_length', type=int, default=128,
                       help='æœ€å¤§ç”Ÿæˆé•¿åº¦')
    parser.add_argument('--device', type=str, default='auto',
                       choices=['auto', 'cuda', 'cpu'],
                       help='è¿è¡Œè®¾å¤‡')
    
    args = parser.parse_args()
    
    # ç¡®å®šè®¾å¤‡
    if args.device == 'auto':
        device = 'cuda' if torch.cuda.is_available() else 'cpu'
    else:
        device = args.device
    
    # åˆ›å»ºç¿»è¯‘å™¨
    translator = Translator(args.model_path, device=device)
    
    # æ‰§è¡Œç¿»è¯‘
    translation = translator.translate(
        text=args.text,
        src_lang=args.src_lang,
        tgt_lang=args.tgt_lang,
        beam_size=args.beam_size,
        max_length=args.max_length
    )
    
    # è¾“å‡ºç»“æœ
    print("="*60)
    print(f"æºè¯­è¨€ ({args.src_lang}): {args.text}")
    print(f"ç›®æ ‡è¯­è¨€ ({args.tgt_lang}): {translation}")
    print("="*60)


if __name__ == "__main__":
    main()

