# Transformer ç¥ç»æœºå™¨ç¿»è¯‘é¡¹ç›®



æœ¬é¡¹ç›®ä»é›¶å®ç°äº†å®Œæ•´çš„ Transformer ç¼–ç å™¨-è§£ç å™¨æ¶æ„ï¼Œç”¨äºè‹±å¾·åŒå‘æœºå™¨ç¿»è¯‘ä»»åŠ¡ã€‚å®ç°åŸºäºåŸå§‹è®ºæ–‡ "[Attention is All You Need](https://arxiv.org/abs/1706.03762)"ï¼Œå¹¶é›†æˆäº†å¤šé¡¹ç°ä»£æ”¹è¿›æŠ€æœ¯ã€‚

## âœ¨ ä¸»è¦ç‰¹æ€§

- ğŸ”¥ **å®Œæ•´å®ç°**ï¼šåŒ…å«ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›ã€å¤šå¤´æ³¨æ„åŠ›ã€ä½ç½®å‰é¦ˆç½‘ç»œã€æ®‹å·®è¿æ¥å’Œå±‚å½’ä¸€åŒ–
- ğŸš€ **ç°ä»£æ”¹è¿›**ï¼š
  - T5é£æ ¼ç›¸å¯¹ä½ç½®ç¼–ç ï¼ˆæ›¿ä»£ç»å¯¹ä½ç½®ç¼–ç ï¼‰
  - Pre-Layer Normalizationï¼ˆæå‡è®­ç»ƒç¨³å®šæ€§ï¼‰
  - Label Smoothingã€Gradient Clippingã€Warmupè°ƒåº¦
- ğŸ’¡ **é«˜æ•ˆæ³¨æ„åŠ›**ï¼šå®ç°äº†ç¨€ç–æ³¨æ„åŠ›ã€çº¿æ€§æ³¨æ„åŠ›ã€Performerç­‰æœºåˆ¶
- ğŸ“Š **å®éªŒå®Œå–„**ï¼šæ”¯æŒå¤šç§å¯¹æ¯”å®éªŒå’Œæ¶ˆèå®éªŒ
- ğŸ¯ **é«˜æ€§èƒ½**ï¼šåœ¨IWSLT2017æ•°æ®é›†ä¸Šè¾¾åˆ°8.20 PPLï¼Œ60.71%å‡†ç¡®ç‡
- ğŸ”§ **æ˜“äºä½¿ç”¨**ï¼šæ¨¡å—åŒ–è®¾è®¡ï¼Œä¸€é”®å¯åŠ¨è®­ç»ƒï¼Œå®Œæ•´æ—¥å¿—å’Œå¯è§†åŒ–

## ğŸ“ é¡¹ç›®ç»“æ„

```
Transformer/
â”œâ”€â”€ src/                          # æ ¸å¿ƒæºä»£ç 
â”‚   â”œâ”€â”€ models/                   # æ¨¡å‹æ¶æ„å®ç°
â”‚   â”‚   â”œâ”€â”€ transformer.py        # ä¸»æ¨¡å‹ï¼ˆç¼–ç å™¨-è§£ç å™¨ï¼‰
â”‚   â”‚   â”œâ”€â”€ encoder.py            # Transformerç¼–ç å™¨
â”‚   â”‚   â”œâ”€â”€ decoder.py            # Transformerè§£ç å™¨
â”‚   â”‚   â”œâ”€â”€ attention.py          # æ³¨æ„åŠ›æœºåˆ¶
â”‚   â”‚   â”œâ”€â”€ relative_positional_encoding.py  # T5ç›¸å¯¹ä½ç½®ç¼–ç 
â”‚   â”‚   â”œâ”€â”€ ffn.py                # ä½ç½®å‰é¦ˆç½‘ç»œ
â”‚   â”‚   â”œâ”€â”€ sparse_attention.py   # ç¨€ç–æ³¨æ„åŠ›
â”‚   â”‚   â””â”€â”€ linear_attention.py   # çº¿æ€§æ³¨æ„åŠ›ä¸Performer
â”‚   â”œâ”€â”€ utils/                    # å·¥å…·å‡½æ•°
â”‚   â”‚   â”œâ”€â”€ data_utils.py         # æ•°æ®å¤„ç†å·¥å…·
â”‚   â”‚   â””â”€â”€ plot_utils.py         # å¯è§†åŒ–å·¥å…·
â”‚   â”œâ”€â”€ train.py                  # ä¸»è®­ç»ƒè„šæœ¬
â”‚   â””â”€â”€ data_process.py           # æ•°æ®åŠ è½½ä¸é¢„å¤„ç†
â”‚
â”œâ”€â”€ scripts/                      # å®éªŒè„šæœ¬
â”‚   â”œâ”€â”€ run.sh                    # ä¸»å®éªŒå¯åŠ¨è„šæœ¬
â”‚   â”œâ”€â”€ experiments.sh            # æ‰¹é‡å¯¹æ¯”å®éªŒ
â”‚   â””â”€â”€ run_translate.sh          # ç¿»è¯‘æ¨ç†è„šæœ¬
â”‚
â”œâ”€â”€ results/                      # ä¸»å®éªŒç»“æœï¼ˆè‡ªåŠ¨ç”Ÿæˆï¼‰
â”œâ”€â”€ data/                         # æ•°æ®å¤„ç†ç»“æœ
â”œâ”€â”€ figures/                      # ä¸åŒnum_headsç»“æœå¯¹æ¯”æŠ¥å‘Šå›¾
â”œâ”€â”€ translator.py                 # äº¤äº’å¼ç¿»è¯‘å·¥å…·
â”œâ”€â”€ requirements.txt              # Pythonä¾èµ–
â””â”€â”€ README.md                     # æœ¬æ–‡ä»¶
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. ç¯å¢ƒé…ç½®

**ä½¿ç”¨ Condaï¼ˆæ¨èï¼‰**
```bash
# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
conda create -n transformer python=3.10
conda activate transformer

# å®‰è£…PyTorchï¼ˆæ ¹æ®ä½ çš„CUDAç‰ˆæœ¬é€‰æ‹©ï¼‰
# CUDA 11.8
conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia

# å®‰è£…å…¶ä»–ä¾èµ–
pip install -r requirements.txt
```

**ä½¿ç”¨ pip**
```bash
pip install -r requirements.txt
```

### 2. æ•°æ®å‡†å¤‡
å»ºè®®ï¼šå°†æ•°æ®é›†ä¸‹è½½åˆ°æœ¬åœ°ï¼Œä¹‹åè¿è¡Œä¸‹è¿°ä»£ç 
```bash
python src/data_process.py
```

### 3. è®­ç»ƒæ¨¡å‹

**æ–¹å¼1ï¼šä½¿ç”¨è„šæœ¬ï¼ˆæ¨èï¼‰**
```bash
bash scripts/run.sh
```

**æ–¹å¼2ï¼šç›´æ¥è¿è¡ŒPython**
```bash
python src/train.py \
    --batch_size 32 \
    --num_epochs 20 \
    --lr 5e-4 \
    --d_model 512 \
    --num_heads 8 \
    --num_layers 6 \
    --d_ff 2048 \
    --dropout 0.1 \
    --max_seq_length 128
```

### 4. ç¿»è¯‘æ¨ç†

**äº¤äº’å¼ç¿»è¯‘**
```bash
python translator.py --checkpoint results/checkpoints/best_model.pt
```

**æ‰¹é‡ç¿»è¯‘**
```bash
bash scripts/run_translate.sh
```

## ğŸ™ è‡´è°¢

æ„Ÿè°¢ä»¥ä¸‹å¼€æºé¡¹ç›®ï¼š
- PyTorch
- Hugging Face Transformers
- IWSLTæ•°æ®é›†

---

**æ³¨æ„**ï¼šæœ¬é¡¹ç›®ç”¨äºå­¦ä¹ å’Œç ”ç©¶ç›®çš„ã€‚å¦‚éœ€åœ¨ç”Ÿäº§ç¯å¢ƒä½¿ç”¨ï¼Œè¯·è¿›è¡Œå……åˆ†çš„æµ‹è¯•å’Œä¼˜åŒ–ã€‚
