#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
IWSLT2017 æ•°æ®å¤„ç†è„šæœ¬
å¤„ç†è‹±å¾·(en-de)åŒå‘ç¿»è¯‘æ•°æ®é›†
"""

import os
import sys
import tarfile
import random
from pathlib import Path


def extract_iwslt_data(tar_path, extract_dir):
    """
    è§£å‹IWSLT2017æ•°æ®é›†
    
    Args:
        tar_path: tar.gzæ–‡ä»¶è·¯å¾„
        extract_dir: è§£å‹ç›®æ ‡ç›®å½•
    
    Returns:
        (en_file_path, de_file_path): è§£å‹åçš„æ–‡ä»¶è·¯å¾„
    """
    print(f"ğŸ“¦ è§£å‹æ•°æ®é›†: {tar_path}")
    
    # ç¡®ä¿ç›®å½•å­˜åœ¨
    os.makedirs(extract_dir, exist_ok=True)
    
    # è§£å‹
    with tarfile.open(tar_path, 'r:gz') as tar:
        tar.extractall(extract_dir)
    
    # æŸ¥æ‰¾è§£å‹åçš„æ–‡ä»¶
    en_file = os.path.join(extract_dir, 'train.en')
    de_file = os.path.join(extract_dir, 'train.de')
    
    if not os.path.exists(en_file) or not os.path.exists(de_file):
        raise FileNotFoundError(f"è§£å‹åæœªæ‰¾åˆ°train.enæˆ–train.deæ–‡ä»¶")
    
    print(f"âœ… è§£å‹å®Œæˆ")
    print(f"   - {en_file}")
    print(f"   - {de_file}")
    
    return en_file, de_file


def load_parallel_data(en_file, de_file, min_len=5, max_len=150):
    """
    åŠ è½½å¹³è¡Œè¯­æ–™ï¼Œè¿›è¡ŒåŸºæœ¬è¿‡æ»¤
    
    Args:
        en_file: è‹±æ–‡æ–‡ä»¶è·¯å¾„
        de_file: å¾·æ–‡æ–‡ä»¶è·¯å¾„
        min_len: æœ€å°å­—ç¬¦é•¿åº¦ï¼ˆè¿‡æ»¤å¤ªçŸ­çš„å¥å­ï¼‰
        max_len: æœ€å¤§å­—ç¬¦é•¿åº¦ï¼ˆè¿‡æ»¤å¤ªé•¿çš„å¥å­ï¼‰
    
    Returns:
        List[tuple]: [(en_text, de_text), ...]
    """
    print(f"\nğŸ“– åŠ è½½å¹³è¡Œè¯­æ–™...")
    
    pairs = []
    
    with open(en_file, 'r', encoding='utf-8') as f_en, \
         open(de_file, 'r', encoding='utf-8') as f_de:
        
        for line_num, (en_line, de_line) in enumerate(zip(f_en, f_de), 1):
            en_text = en_line.strip()
            de_text = de_line.strip()
            
            # è¿‡æ»¤ç©ºè¡Œ
            if not en_text or not de_text:
                continue
            
            # è¿‡æ»¤é•¿åº¦ä¸åˆé€‚çš„å¥å­
            if len(en_text) < min_len or len(de_text) < min_len:
                continue
            if len(en_text) > max_len * 10 or len(de_text) > max_len * 10:
                continue
            
            pairs.append((en_text, de_text))
            
            if line_num % 50000 == 0:
                print(f"  å·²å¤„ç† {line_num:,} è¡Œ...")
    
    print(f"âœ… åŠ è½½å®Œæˆ: {len(pairs):,} æ¡æœ‰æ•ˆå¥å¯¹")
    return pairs


def create_bidirectional_dataset(pairs, val_ratio=0.1, seed=42, max_pairs=None):
    """
    åˆ›å»ºåŒå‘ç¿»è¯‘æ•°æ®é›†ï¼ˆenâ†’de å’Œ deâ†’enï¼‰
    
    Args:
        pairs: List[(en_text, de_text)]
        val_ratio: éªŒè¯é›†æ¯”ä¾‹
        seed: éšæœºç§å­
        max_pairs: æœ€å¤§å¥å¯¹æ•°é‡ï¼ˆNoneè¡¨ç¤ºä½¿ç”¨å…¨éƒ¨ï¼‰
    
    Returns:
        train_data: List[(src, tgt)]
        val_data: List[(src, tgt)]
    """
    print(f"\nğŸ”€ åˆ›å»ºåŒå‘ç¿»è¯‘æ•°æ®é›†...")
    
    random.seed(seed)
    
    # æ‰“ä¹±æ•°æ®
    pairs_shuffled = pairs.copy()
    random.shuffle(pairs_shuffled)
    
    # é™åˆ¶æ•°æ®é‡
    if max_pairs is not None and max_pairs < len(pairs_shuffled):
        pairs_shuffled = pairs_shuffled[:max_pairs]
        print(f"  âš ï¸  é™åˆ¶æ•°æ®é‡: ä½¿ç”¨ {max_pairs:,} / {len(pairs):,} å¥å¯¹")
    
    # åˆ’åˆ†train/val
    val_size = int(len(pairs_shuffled) * val_ratio)
    train_pairs = pairs_shuffled[val_size:]
    val_pairs = pairs_shuffled[:val_size]
    
    print(f"  åŸå§‹å¥å¯¹: {len(pairs):,}")
    print(f"  Trainå¥å¯¹: {len(train_pairs):,}")
    print(f"  Valå¥å¯¹: {len(val_pairs):,}")
    
    # åˆ›å»ºåŒå‘æ•°æ®ï¼ˆæ¯ä¸ªå¥å¯¹äº§ç”Ÿä¸¤ä¸ªæ ·æœ¬ï¼šenâ†’de å’Œ deâ†’enï¼‰
    train_data = []
    val_data = []
    
    # å¤„ç†è®­ç»ƒé›†
    for en_text, de_text in train_pairs:
        # en â†’ de
        train_data.append((f"<2de> {en_text}", de_text))
        # de â†’ en
        train_data.append((f"<2en> {de_text}", en_text))
    
    # å¤„ç†éªŒè¯é›†
    for en_text, de_text in val_pairs:
        # en â†’ de
        val_data.append((f"<2de> {en_text}", de_text))
        # de â†’ en
        val_data.append((f"<2en> {de_text}", en_text))
    
    # å†æ¬¡æ‰“ä¹±ï¼ˆæ··åˆenâ†’deå’Œdeâ†’enï¼‰
    random.shuffle(train_data)
    random.shuffle(val_data)
    
    print(f"\nğŸ“Š åŒå‘æ•°æ®ç»Ÿè®¡:")
    print(f"  Train: {len(train_data):,} æ ·æœ¬ (enâ†’de: {len(train_pairs):,}, deâ†’en: {len(train_pairs):,})")
    print(f"  Val:   {len(val_data):,} æ ·æœ¬ (enâ†’de: {len(val_pairs):,}, deâ†’en: {len(val_pairs):,})")
    
    return train_data, val_data


def save_to_files(train_data, val_data, output_dir):
    """
    ä¿å­˜å¤„ç†åçš„æ•°æ®åˆ°æ–‡ä»¶
    
    Args:
        train_data: List[(src, tgt)]
        val_data: List[(src, tgt)]
        output_dir: è¾“å‡ºç›®å½•
    """
    print(f"\nğŸ’¾ ä¿å­˜æ•°æ®åˆ°: {output_dir}")
    
    os.makedirs(output_dir, exist_ok=True)
    
    # ä¿å­˜è®­ç»ƒé›†
    train_src_file = os.path.join(output_dir, 'train.src')
    train_tgt_file = os.path.join(output_dir, 'train.tgt')
    
    with open(train_src_file, 'w', encoding='utf-8') as f_src, \
         open(train_tgt_file, 'w', encoding='utf-8') as f_tgt:
        for src, tgt in train_data:
            f_src.write(src + '\n')
            f_tgt.write(tgt + '\n')
    
    print(f"âœ… è®­ç»ƒé›†ä¿å­˜å®Œæˆ:")
    print(f"   - {train_src_file} ({len(train_data):,} è¡Œ)")
    print(f"   - {train_tgt_file} ({len(train_data):,} è¡Œ)")
    
    # ä¿å­˜éªŒè¯é›†
    val_src_file = os.path.join(output_dir, 'val.src')
    val_tgt_file = os.path.join(output_dir, 'val.tgt')
    
    with open(val_src_file, 'w', encoding='utf-8') as f_src, \
         open(val_tgt_file, 'w', encoding='utf-8') as f_tgt:
        for src, tgt in val_data:
            f_src.write(src + '\n')
            f_tgt.write(tgt + '\n')
    
    print(f"âœ… éªŒè¯é›†ä¿å­˜å®Œæˆ:")
    print(f"   - {val_src_file} ({len(val_data):,} è¡Œ)")
    print(f"   - {val_tgt_file} ({len(val_data):,} è¡Œ)")


def data_process(args=None):
    """
    æ•°æ®å¤„ç†å‡½æ•°ï¼ˆå…¼å®¹train.pyè°ƒç”¨ï¼‰
    
    Args:
        args: å‘½ä»¤è¡Œå‚æ•°ï¼ˆå¯é€‰ï¼Œè¿™é‡Œä¸ä½¿ç”¨ï¼‰
    """
    # é…ç½®
    TAR_PATH = "/home/extra_home/lc/IWSLT2017-en-de-v2.tar.gz"
    EXTRACT_DIR = "/home/extra_home/lc/iwslt2017_extracted"
    OUTPUT_DIR = "data"
    VAL_RATIO = 0.1
    SEED = 42
    MAX_PAIRS = 100000  # æœ€å¤§å¥å¯¹æ•°ï¼ˆåŒå‘å train â‰ˆ 18Kæ ·æœ¬ï¼‰
    # MAX_PAIRS = None
    
    # è·å–é¡¹ç›®æ ¹ç›®å½•
    project_root = Path(__file__).parent.parent
    output_dir = project_root / OUTPUT_DIR
    
    # # æ£€æŸ¥æ•°æ®æ˜¯å¦å·²å­˜åœ¨
    # train_src = output_dir / "train.src"
    # train_tgt = output_dir / "train.tgt"
    # val_src = output_dir / "val.src"
    # val_tgt = output_dir / "val.tgt"
    
    # if all(f.exists() for f in [train_src, train_tgt, val_src, val_tgt]):
    #     print("=" * 80)
    #     print("âœ… æ•°æ®æ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡æ•°æ®å¤„ç†")
    #     print("=" * 80)
    #     print(f"  - {train_src} ({sum(1 for _ in open(train_src)):,} è¡Œ)")
    #     print(f"  - {val_src} ({sum(1 for _ in open(val_src)):,} è¡Œ)")
    #     print()
    #     return
    
    print("=" * 80)
    print("ğŸš€ IWSLT2017 è‹±å¾·åŒå‘ç¿»è¯‘æ•°æ®é›†å¤„ç†")
    print("=" * 80)
    
    print(f"\nâš™ï¸  é…ç½®:")
    print(f"  æ•°æ®é›†: {TAR_PATH}")
    print(f"  è§£å‹ç›®å½•: {EXTRACT_DIR}")
    print(f"  è¾“å‡ºç›®å½•: {output_dir}")
    print(f"  éªŒè¯é›†æ¯”ä¾‹: {VAL_RATIO * 100}%")
    # print(f"  æœ€å¤§å¥å¯¹æ•°: {MAX_PAIRS:,} (è®­ç»ƒæ ·æœ¬çº¦ {int(MAX_PAIRS * (1-VAL_RATIO) * 2):,} æ¡)")
    print(f"  éšæœºç§å­: {SEED}")
    print()
    
    # Step 1: è§£å‹æ•°æ®
    en_file, de_file = extract_iwslt_data(TAR_PATH, EXTRACT_DIR)
    
    # Step 2: åŠ è½½å¹³è¡Œè¯­æ–™
    pairs = load_parallel_data(en_file, de_file)
    
    # Step 3: åˆ›å»ºåŒå‘æ•°æ®é›†ï¼ˆé™åˆ¶æ•°æ®é‡ï¼‰
    train_data, val_data = create_bidirectional_dataset(pairs, val_ratio=VAL_RATIO, seed=SEED, max_pairs=MAX_PAIRS)
    
    # Step 4: ä¿å­˜åˆ°æ–‡ä»¶
    save_to_files(train_data, val_data, output_dir)
    
    print("\n" + "=" * 80)
    print("âœ… æ•°æ®å¤„ç†å®Œæˆï¼")
    print("=" * 80)
    print(f"\nğŸ“ æ•°æ®æ ¼å¼è¯´æ˜:")
    print(f"  - æºæ–‡æœ¬åŒ…å«è¯­è¨€æ ‡è®°: <2de> (ç›®æ ‡å¾·è¯­) æˆ– <2en> (ç›®æ ‡è‹±è¯­)")
    print(f"  - æ”¯æŒåŒå‘ç¿»è¯‘: English â†” German")
    print(f"\nğŸ¯ ä¸‹ä¸€æ­¥:")
    print(f"  1. æ£€æŸ¥æ•°æ®: head -n 5 {output_dir}/train.src")
    print(f"  2. å¼€å§‹è®­ç»ƒ: bash scripts/run.sh")
    print()


def main():
    """ä¸»å‡½æ•°ï¼ˆç›´æ¥è°ƒç”¨æ—¶ä½¿ç”¨ï¼‰"""
    data_process()


if __name__ == '__main__':
    try:
        main()
    except Exception as e:
        print(f"\nâŒ é”™è¯¯: {e}", file=sys.stderr)
        import traceback
        traceback.print_exc()
        sys.exit(1)
