"""
è®­ç»ƒè„šæœ¬ï¼šæŠŠæ•°æ®è½½å…¥ã€æ¨¡å‹æ„å»ºã€è®­ç»ƒå¾ªç¯ã€éªŒè¯ä¸ checkpoint ä¿å­˜/åŠ è½½
"""

import os
import math
import time
import random
import argparse
from pathlib import Path

import torch
import torch.nn.functional as F
from torch.utils.data import DataLoader

from src.utils.data_utils import TextPairDataset
from transformers import AutoTokenizer, AdamW, get_linear_schedule_with_warmup,AutoModelForMaskedLM

from src.models.transformer_ablation import TransformerSeq2Seq

import math
import matplotlib.pyplot as plt
from torch.optim import AdamW
from torch.nn.utils import clip_grad_norm_
from torch.optim.lr_scheduler import LambdaLR
from src.utils.plot_utils import plot_training_curves
from src.data_process import data_process

# -------------------------
# å·¥å…·å‡½æ•°
# -------------------------
def set_seed(seed: int):
    random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)

def save_checkpoint(state: dict, save_dir: str, prefix: str = "ckpt"):
    os.makedirs(save_dir, exist_ok=True)
    path = os.path.join(save_dir, f"{prefix}.pt")
    torch.save(state, path)
    print(f"Saved checkpoint to {path}")

def load_checkpoint(model, optimizer, scheduler, ckpt_path: str, device):
    checkpoint = torch.load(ckpt_path, map_location=device)
    model.load_state_dict(checkpoint["model_state_dict"])
    optimizer.load_state_dict(checkpoint["optim_state_dict"])
    if "scheduler_state_dict" in checkpoint and scheduler is not None:
        scheduler.load_state_dict(checkpoint["scheduler_state_dict"])
    start_epoch = checkpoint.get("epoch", 0)
    print(f"Loaded checkpoint {ckpt_path} (start_epoch={start_epoch})")
    return start_epoch

# -------------------------
# collate_fnï¼šæŠŠåŸå§‹ dataset ä¸­çš„ tokenized outputs batch åŒ–
# è¿™é‡Œé‡‡ç”¨ tokenizer.batch_encode_plus åŠ¨æ€ paddingï¼Œé¿å…å›ºå®š max_length å¯¼è‡´ç©ºé—´æµªè´¹
# -------------------------
def collate_fn(batch, tokenizer, max_src_len=128, max_tgt_len=128):
    """
    batch: list of dicts returned by TranslationDataset (æº/ç›®æ ‡åŸæ–‡æœ¬ï¼Œæˆ–å·² tokenized)
    è¿”å›ï¼š
      src_input_ids, src_attention_mask, tgt_input_ids (decoder inputs), labels
    labels ä¸­ pad éƒ¨åˆ†ç”¨ -100ï¼ˆCrossEntropy çš„ ignore_indexï¼‰
    """
    src_texts = [ex["translation"][tokenizer.model_input_names[0].split("_")[0] if False else 'src'] 
                 if "translation" not in ex else ex["translation"].get("src", None) for ex in batch]
    # å‡è®¾ batch ä¸­çš„ item æ˜¯ dict: {"src_text":..., "tgt_text":...}

    if isinstance(batch[0], dict) and "src_text" in batch[0] and "tgt_text" in batch[0]:
        src_texts = [ex["src_text"] for ex in batch]
        tgt_texts = [ex["tgt_text"] for ex in batch]
    else:
        src_texts = []
        tgt_texts = []
        for ex in batch:
            if "translation" in ex:
                trans = ex["translation"]
                keys = list(trans.keys())
                if len(keys) >= 2:
                    src_texts.append(trans[keys[0]])
                    tgt_texts.append(trans[keys[1]])
                else:
                    raise ValueError("translation dict has <2 languages")
            else:
                raise ValueError("Batch format not supported by collate_fn. Expected keys 'src_text'/'tgt_text' or 'translation' dict.")

    # tokenizer.batch_encode_plus æ”¯æŒåŠ¨æ€ padding
    src_enc = tokenizer(src_texts, return_tensors="pt", padding=True, truncation=True, max_length=max_src_len)
    tgt_enc = tokenizer(tgt_texts, return_tensors="pt", padding=True, truncation=True, max_length=max_tgt_len)

    src_input_ids = src_enc["input_ids"]
    src_attention_mask = src_enc["attention_mask"]
    tgt_input_ids = tgt_enc["input_ids"]  # è¿™åŒ…å« [CLS] ... [SEP] [PAD]

    # æ„é€  decoder è¾“å…¥ï¼šå°† labels å³ç§»ä¸€ä½å¹¶åœ¨å¼€å¤´å¡« BOS (ä½¿ç”¨ tokenizer.cls_token_id)
    # labels: çœŸå®ç›®æ ‡ tokensï¼ˆç”¨äºè®¡ç®— lossï¼‰
    labels = tgt_input_ids.clone()
    pad_token_id = tokenizer.pad_token_id
    cls_token_id = tokenizer.cls_token_id if tokenizer.cls_token_id is not None else tokenizer.bos_token_id

    # prepare decoder_input_ids by shifting right
    decoder_input_ids = torch.full(labels.size(), pad_token_id, dtype=torch.long)
    decoder_input_ids[:, 0] = cls_token_id
    decoder_input_ids[:, 1:] = labels[:, :-1].clone()

    # å°† labels ä¸­ pad token æ›¿æ¢ä¸º -100ï¼Œä»¥ä¾¿ CrossEntropy å¿½ç•¥
    labels_masked = labels.masked_fill(labels == pad_token_id, -100)

    batch_out = {
        "src_input_ids": src_input_ids,
        "src_attention_mask": src_attention_mask,
        "tgt_input_ids": decoder_input_ids,
        "labels": labels_masked,
        "raw_labels": labels,  # æ–¹ä¾¿è°ƒè¯•
        "pad_token_id": pad_token_id
    }
    return batch_out

# -------------------------
# è®­ç»ƒä¸éªŒè¯å‡½æ•°
# -------------------------
def evaluate(model, dataloader, tokenizer, device):
    model.eval()
    total_loss = 0.0
    total_tokens = 0
    with torch.no_grad():
        for batch in dataloader:
            src = batch["src_input_ids"].to(device)
            tgt_in = batch["tgt_input_ids"].to(device)
            labels = batch["labels"].to(device)

            logits = model(src, tgt_in, src_pad_id=batch["pad_token_id"], tgt_pad_id=batch["pad_token_id"])
            B, T, V = logits.size()
            loss = F.cross_entropy(logits.view(-1, V), labels.view(-1), ignore_index=-100, reduction="sum")
            n_tokens = (labels != -100).sum().item()

            total_loss += loss.item()
            total_tokens += n_tokens
    avg_loss = total_loss / total_tokens
    ppl = math.exp(avg_loss)
    return avg_loss, ppl

def train(args):
    # 1. éšæœºç§å­
    set_seed(args.seed)

    # 2. è®¾å¤‡
    device = torch.device("cuda:1" if torch.cuda.is_available() and args.cuda else "cpu")
    print("Device:", device)

    # 3. æ•°æ®å¤„ç†ï¼ˆå¦‚æœæ•°æ®æ–‡ä»¶ä¸å­˜åœ¨åˆ™è‡ªåŠ¨ä¸‹è½½å’Œå¤„ç†ï¼‰
    # ä½¿ç”¨é€šç”¨æ–‡ä»¶åï¼ˆåŒ…å«æ‰€æœ‰è¯­è¨€å¯¹çš„äº’è¯‘ï¼‰
    src_path = "data/train.src"  # å¤šè¯­è¨€æºæ–‡æœ¬
    tgt_path = "data/train.tgt"  # å¤šè¯­è¨€ç›®æ ‡æ–‡æœ¬
    
    if not (Path(src_path).exists() and Path(tgt_path).exists()):
        print(f"\nğŸ“¥ æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨ï¼Œå¼€å§‹å¤„ç†æ•°æ®é›†...")
        print(f"   ğŸŒ å°†åŠ è½½TED Talkså¤šè¯­è¨€æ•°æ®é›†ï¼ˆæ‰€æœ‰è¯­è¨€å¯¹äº’è¯‘ï¼Œæ‰€æœ‰å¹´ä»½ï¼‰")
        print(f"   ğŸ“Š è¿™å°†åŒ…å«109ç§è¯­è¨€ä¹‹é—´çš„æ‰€æœ‰å¯èƒ½ç¿»è¯‘ç»„åˆ")
        print(f"   ğŸ’¾ é¢„è®¡æ•°æ®é‡ï¼š500K-1Mæ¡")
        print(f"   â³ é¦–æ¬¡åŠ è½½éœ€è¦è¾ƒé•¿æ—¶é—´ï¼ˆçº¦30-60åˆ†é’Ÿï¼‰ï¼Œè¯·è€å¿ƒç­‰å¾…...\n")
        data_process(args)
        print(f"\nâœ… æ•°æ®å¤„ç†å®Œæˆï¼\n")
    else:
        print(f"âœ… å‘ç°å·²æœ‰æ•°æ®æ–‡ä»¶: {src_path}, {tgt_path}")
        print(f"   æ³¨æ„ï¼šè¿™æ˜¯å¤šè¯­è¨€å¤šå‘æ•°æ®ï¼ˆ109ç§è¯­è¨€äº’è¯‘ï¼‰\n")

    # 4. tokenizerï¼ˆä»æœ¬åœ°è·¯å¾„åŠ è½½ï¼‰
    # 4. åŠ è½½tokenizerå¹¶æ·»åŠ è¯­è¨€æ ‡è®°
    print("\n" + "="*60)
    print("4. åŠ è½½å¹¶é…ç½®Tokenizer")
    print("="*60)
    
    tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_path, local_files_only=True)
    original_vocab_size = len(tokenizer)
    print(f"åŸå§‹vocab size: {original_vocab_size}")
    
    # æ·»åŠ è¯­è¨€æ ‡è®°ä½œä¸ºç‰¹æ®Štokenï¼ˆè¿™æ ·ä¼šè¢«tokenizeæˆå•ä¸ªtokenï¼‰
    special_tokens = ['<2en>', '<2de>']  # è‹±è¯­å’Œå¾·è¯­
    num_added = tokenizer.add_special_tokens({'additional_special_tokens': special_tokens})
    
    new_vocab_size = len(tokenizer)
    print(f"æ·»åŠ  {num_added} ä¸ªè¯­è¨€æ ‡è®°: {special_tokens}")
    print(f"æ–°vocab size: {new_vocab_size}")

    # ä¿å­˜æ›´æ–°åçš„tokenizeråˆ°ä¸´æ—¶ç›®å½•ï¼Œç¡®ä¿Datasetä½¿ç”¨ç›¸åŒçš„tokenizer
    tokenizer_with_tags_path = "data/tokenizer_with_tags"
    os.makedirs(tokenizer_with_tags_path, exist_ok=True)
    tokenizer.save_pretrained(tokenizer_with_tags_path)
    print(f"âœ… å·²ä¿å­˜æ›´æ–°åçš„tokenizeråˆ°: {tokenizer_with_tags_path}")
    
    # éªŒè¯ç‰¹æ®Štokenæ˜¯å¦æ­£ç¡®æ·»åŠ 
    print("\néªŒè¯è¯­è¨€æ ‡è®°:")
    for tag in special_tokens:
        token_id = tokenizer.convert_tokens_to_ids(tag)
        decoded = tokenizer.decode([token_id])
        print(f"  {tag} â†’ token_id={token_id} â†’ decoded='{decoded}'")
    
    vocab_size = new_vocab_size

    # 5. ä»æœ¬åœ° train.src / train.tgt åŠ è½½æ•°æ®
    print("\n" + "="*60)
    print("5. åŠ è½½è®­ç»ƒæ•°æ®")
    print("="*60)

    dataset = TextPairDataset(
        src_path=src_path,
        tgt_path=tgt_path,
        tokenizer_path=tokenizer_with_tags_path,  # ä½¿ç”¨åŒ…å«è¯­è¨€æ ‡è®°çš„tokenizer
        max_src_len=args.max_src_len,
        max_tgt_len=args.max_tgt_len
    )

    # éšæœºåˆ’åˆ†è®­ç»ƒ/éªŒè¯
    total_size = len(dataset)
    val_split = int(total_size * args.val_ratio)
    indices = list(range(total_size))
    random.shuffle(indices)
    val_indices = indices[:val_split]
    train_indices = indices[val_split:]

    from torch.utils.data import Subset
    train_dataset = Subset(dataset, train_indices)
    val_dataset = Subset(dataset, val_indices)

    print(f"Train examples: {len(train_dataset)}, Val examples: {len(val_dataset)}")

    train_loader = DataLoader(
        train_dataset,
        batch_size=args.batch_size,
        shuffle=True,
        collate_fn=dataset.collate_fn
    )
    val_loader = DataLoader(
        val_dataset,
        batch_size=args.eval_batch_size,
        shuffle=False,
        collate_fn=dataset.collate_fn
    )

    # 5. æ¨¡å‹ï¼ˆä½¿ç”¨T5é£æ ¼çš„ç›¸å¯¹ä½ç½®ç¼–ç ï¼‰
    model = TransformerSeq2Seq(
        vocab_size=vocab_size, 
        d_model=args.d_model, 
        num_layers=args.num_layers,
        num_heads=args.num_heads, 
        d_ff=args.d_ff, 
        max_len=args.max_src_len, 
        dropout=args.dropout,
        share_embeddings=args.share_embeddings,
        attention_type=args.attention_type
    )
    model = model.to(device)
    
    # å‚æ•°ç»Ÿè®¡
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"Total parameters: {total_params:,} | Trainable: {trainable_params:,}")
    print(f"Position Encoding: T5-style Relative Position Bias")
    print(f"Attention Type: {args.attention_type}")


    # 6. ä¼˜åŒ–å™¨ + scheduler
    optimizer = AdamW(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)
    total_steps = len(train_loader) * args.epochs
    print("total_steps: ", total_steps)
    warmup_steps = int(total_steps * args.warmup_ratio)
    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps)

    # 7. æ¨¡å‹è®­ç»ƒ
    global_step = 0
    train_losses, val_losses = [], []
    train_ppls, val_ppls = [], []
    val_accuracies = []

    best_val_loss = float('inf')
    best_epoch = 0
    patience_counter = 0  # Early stopping è®¡æ•°å™¨

    for epoch in range(args.epochs):
        model.train()
        total_loss = 0.0
        total_tokens = 0

        for step, batch in enumerate(train_loader):
            src_input_ids = batch["src_input_ids"].to(device)
            src_attention_mask = batch["src_attention_mask"].to(device)
            tgt_input_ids = batch["tgt_input_ids"].to(device)
            labels = batch["labels"].to(device)

            outputs = model(
                src_input_ids=src_input_ids,
                src_attention_mask=src_attention_mask,
                tgt_input_ids=tgt_input_ids,
                labels=labels,
                label_smoothing=args.label_smoothing
            )

            loss = outputs.loss
            total_loss += loss.item()

            loss.backward()

            # æ¢¯åº¦è£å‰ªï¼ˆé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸ï¼‰
            clip_grad_norm_(model.parameters(), max_norm=1.0)

            optimizer.step()
            scheduler.step()
            optimizer.zero_grad()

            global_step += 1
            if step % 100 == 0:
                lr = scheduler.get_last_lr()[0]
                print(f"[Epoch {epoch+1}] Step {step:04d} | Loss: {loss.item():.4f} | LR: {lr:.6f}")

        # ====== Trainingç»Ÿè®¡ ======
        # é™¤ä»¥batchæ•°é‡ï¼Œå› ä¸ºæ¯ä¸ªlosså·²ç»æ˜¯batchå†…çš„å¹³å‡
        avg_train_loss = total_loss / len(train_loader)
        train_losses.append(avg_train_loss)
        train_ppl = math.exp(avg_train_loss)
        train_ppls.append(train_ppl)

        # ====== Validation ======
        model.eval()
        val_loss = 0.0
        val_correct = 0
        val_total_tokens = 0

        with torch.no_grad():
            for batch in val_loader:
                src_input_ids = batch["src_input_ids"].to(device)
                src_attention_mask = batch["src_attention_mask"].to(device)
                tgt_input_ids = batch["tgt_input_ids"].to(device)
                labels = batch["labels"].to(device)

                outputs = model(
                    src_input_ids=src_input_ids,
                    src_attention_mask=src_attention_mask,
                    tgt_input_ids=tgt_input_ids,
                    labels=labels,
                    label_smoothing=0.0  # éªŒè¯æ—¶ä¸ä½¿ç”¨label smoothing
                )
                val_loss += outputs.loss.item()

                # ====== token-level accuracy ======
                predictions = outputs.logits.argmax(dim=-1)  # (B, T_tgt)
                mask = (labels != -100)
                val_correct += ((predictions == labels) & mask).sum().item()
                val_total_tokens += mask.sum().item()

        # é™¤ä»¥batchæ•°é‡ï¼Œå› ä¸ºæ¯ä¸ªlosså·²ç»æ˜¯batchå†…çš„å¹³å‡
        avg_val_loss = val_loss / len(val_loader)
        val_losses.append(avg_val_loss)
        val_ppl = math.exp(avg_val_loss)
        val_ppls.append(val_ppl)
        val_accuracy = val_correct / val_total_tokens if val_total_tokens > 0 else 0.0
        val_accuracies.append(val_accuracy)

        print(f"\nğŸ“˜ Epoch {epoch+1} Summary:")
        print(f"  Train Loss={avg_train_loss:.4f}, Train PPL={train_ppl:.2f}")
        print(f"  Val   Loss={avg_val_loss:.4f}, Val PPL={val_ppl:.2f}, Val Acc={val_accuracy:.4f}\n")

        # ====== å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹ï¼ˆæ¯20ä¸ªepochæˆ–æœ€åä¸€ä¸ªepochï¼‰======
        if (epoch + 1) % 20 == 0 or (epoch + 1) == args.epochs:
            ckpt_dir = os.path.join(args.output_dir, "checkpoints")
            os.makedirs(ckpt_dir, exist_ok=True)
            ckpt_path = os.path.join(ckpt_dir, f"checkpoint_epoch{epoch+1}.pt")
            torch.save({
                "epoch": epoch,
                "model_state": model.state_dict(),
                "optimizer_state": optimizer.state_dict(),
                "scheduler_state": scheduler.state_dict(),
                "train_loss": avg_train_loss,
                "val_loss": avg_val_loss
            }, ckpt_path)
            print(f"âœ… å®šæœŸæ£€æŸ¥ç‚¹å·²ä¿å­˜è‡³ {ckpt_path}")

        # ====== æœ€ä½³æ¨¡å‹è¿½è¸ª & Early Stopping ======
        if avg_val_loss < best_val_loss:
            best_val_loss = avg_val_loss
            best_epoch = epoch + 1
            patience_counter = 0  # é‡ç½®early stoppingè®¡æ•°å™¨
            best_path = os.path.join(args.output_dir, "best_model.pt")
            
            # ä¿å­˜checkpointï¼ˆä½¿ç”¨åŒ…å«è¯­è¨€æ ‡è®°çš„tokenizerè·¯å¾„ï¼‰
            checkpoint_args = vars(args).copy()
            checkpoint_args['tokenizer_path'] = tokenizer_with_tags_path  # ä½¿ç”¨æ›´æ–°åçš„tokenizerè·¯å¾„
            
            torch.save({
                "epoch": epoch,
                "model_state": model.state_dict(),
                "optimizer_state": optimizer.state_dict(),
                "scheduler_state": scheduler.state_dict(),
                "val_loss": avg_val_loss,
                "tokenizer": tokenizer_with_tags_path,  # ä¿å­˜åŒ…å«è¯­è¨€æ ‡è®°çš„tokenizerè·¯å¾„
                "args": checkpoint_args,
            }, best_path)
            print(f"ğŸ† æœ€ä½³æ¨¡å‹æ›´æ–°å¹¶ä¿å­˜è‡³ {best_path}")
        else:
            patience_counter += 1
            print(f"âš ï¸  éªŒè¯lossæœªæ”¹å–„ ({patience_counter}/{args.patience})")
            
            if patience_counter >= args.patience:
                print(f"\nğŸ›‘ Early Stopping è§¦å‘ï¼")
                print(f"   æœ€ä½³Epoch: {best_epoch}, Val Loss={best_val_loss:.4f}, Val PPL={math.exp(best_val_loss):.2f}")
                print(f"   å½“å‰Epoch: {epoch+1}, Val Loss={avg_val_loss:.4f}, Val PPL={val_ppl:.2f}")
                break

    print(f"\nâœ… è®­ç»ƒå®Œæˆï¼")
    print(f"   æœ€ä½³æ¨¡å‹: Epoch {best_epoch}, Val Loss={best_val_loss:.4f}, Val PPL={math.exp(best_val_loss):.2f}")
    print(f"   æ‰€æœ‰ç»“æœä¸æ—¥å¿—å·²ä¿å­˜åœ¨ results/")


    plot_training_curves(train_losses, val_losses, save_path=os.path.join(args.output_dir, "training_curve.png"))
    
    # ä¿å­˜æ‰€æœ‰æŒ‡æ ‡åˆ°æ–‡ä»¶
    metrics_file = os.path.join(args.output_dir, "metrics.txt")
    with open(metrics_file, 'w') as f:
        f.write("Epoch\tTrain_Loss\tTrain_PPL\tVal_Loss\tVal_PPL\tVal_Accuracy\n")
        for i in range(len(train_losses)):
            f.write(f"{i+1}\t{train_losses[i]:.4f}\t{train_ppls[i]:.4f}\t"
                   f"{val_losses[i]:.4f}\t{val_ppls[i]:.4f}\t{val_accuracies[i]:.4f}\n")
    print(f"Metrics saved to {metrics_file}")

def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument("--src_lang", type=str, default="nl", help="source language code")
    parser.add_argument("--tgt_lang", type=str, default="en", help="target language code")
    parser.add_argument("--tokenizer_path", type=str, default="/home/extra_home/lc/google-bert/rembert", help="local tokenizer path")
    parser.add_argument("--batch_size", type=int, default=16)
    parser.add_argument("--eval_batch_size", type=int, default=32)
    parser.add_argument("--epochs", type=int, default=5)
    parser.add_argument("--val_ratio", type=float, default=0.05, help="ç”¨äºæŠ½å–å°éªŒè¯é›†æ¯”ä¾‹")
    parser.add_argument("--max_src_len", type=int, default=128)
    parser.add_argument("--max_tgt_len", type=int, default=128)
    parser.add_argument("--d_model", type=int, default=256)
    parser.add_argument("--num_layers", type=int, default=4)
    parser.add_argument("--num_heads", type=int, default=4)
    parser.add_argument("--d_ff", type=int, default=512)
    parser.add_argument("--dropout", type=float, default=0.1)
    parser.add_argument("--share_embeddings", action="store_true")

    # æ³¨æ„åŠ›æœºåˆ¶ç±»å‹å‚æ•°
    parser.add_argument("--attention_type", type=str, default="standard",
                       choices=["standard", "local_sparse", "strided_sparse", "block_sparse", "linear", "causal_linear", "performer"],
                       help="æ³¨æ„åŠ›æœºåˆ¶ç±»å‹: standardï¼ˆæ ‡å‡†O(n^2)ï¼Œä½¿ç”¨T5é£æ ¼ç›¸å¯¹ä½ç½®ç¼–ç ï¼‰, local_sparseï¼ˆå±€éƒ¨ç¨€ç–ï¼‰, strided_sparseï¼ˆè·¨æ­¥ç¨€ç–ï¼‰, block_sparseï¼ˆå—ç¨€ç–ï¼‰, linearï¼ˆçº¿æ€§O(n)ï¼‰, causal_linearï¼ˆå› æœçº¿æ€§ï¼Œç”¨äºdecoderï¼‰, performerï¼ˆPerformerï¼‰")
    
    parser.add_argument("--lr", type=float, default=5e-4)
    parser.add_argument("--weight_decay", type=float, default=0.01)
    parser.add_argument("--warmup_ratio", type=float, default=0.06)
    parser.add_argument("--grad_clip", type=float, default=1.0)
    parser.add_argument("--log_steps", type=int, default=200)
    parser.add_argument("--output_dir", type=str, default="results")
    parser.add_argument("--seed", type=int, default=42)
    parser.add_argument("--subset_size", type=int, default=0, help="æ•°æ®å­é›†å¤§å°ï¼ˆ0=ä½¿ç”¨å…¨éƒ¨æ•°æ®ï¼Œ>0=ä½¿ç”¨æŒ‡å®šæ•°é‡ï¼‰")
    parser.add_argument("--cuda", action="store_true", help="if set, use cuda when available")
    
    # æ­£åˆ™åŒ–ä¸æ—©åœå‚æ•°
    parser.add_argument("--patience", type=int, default=10, help="Early stopping: éªŒè¯lossä¸æ”¹å–„çš„å®¹å¿epochæ•°")
    parser.add_argument("--label_smoothing", type=float, default=0.0, help="Label smoothing factor (0.0-0.2)")
    
    return parser.parse_args()

if __name__ == "__main__":
    args = parse_args()
    # è‡ªåŠ¨å¤„ç†æ•°æ®ï¼ˆå¦‚æœæ•°æ®ä¸å­˜åœ¨ï¼‰
    # data_process(args)
    # å¼€å§‹è®­ç»ƒ
    train(args)

